{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3. Word Embedding",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hw-tan/Capstone-Project/blob/main/3_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx6NrhLr-HyD"
      },
      "source": [
        "BERT, or Bidirectional Embedding Representations from Transformers is a language representation model that is pre-trained from a huge amount of plain text on the web. The model can be fine-tuned with an additional output layer, and can handle a wide range of task [Read more](https://arxiv.org/abs/1810.04805). \n",
        "\n",
        "LaBSE, Language-agnostic BERT Sentence Embedding is the multilingual adaptation of BERT. [Read more](https://arxiv.org/abs/2007.01852)\n",
        "\n",
        "In this notebook, we process the title data (description of item) from the Shopee dataset to fit it into a pre-train BERT model to extract word embeddings of the titles.\n",
        "\n",
        "With the word embeddings we will determine its Nearest Neighbors and select a cut-off score to determine what other titles can be classified as duplicates.\n",
        "\n"
      ],
      "id": "Qx6NrhLr-HyD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMPU94omBHFj",
        "outputId": "cbc10649-f69e-413d-bee4-6ba6efbd5ecc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) "
      ],
      "id": "ZMPU94omBHFj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA40UsCOU_aI",
        "outputId": "faec6b8d-aed5-4eba-b8d5-085005a7b4d4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "LA40UsCOU_aI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug  4 04:02:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gw6cSwipY-B"
      },
      "source": [
        "# New Section"
      ],
      "id": "1Gw6cSwipY-B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya5jnsLEFp_w",
        "outputId": "757146d1-40b6-4543-d2b6-cc06df7f9cc6"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "\n",
        "import bert"
      ],
      "id": "Ya5jnsLEFp_w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e72099b"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Neural Network\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "\n",
        "\n",
        "#NLP libraries\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "id": "3e72099b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cVIWzzJFS3f"
      },
      "source": [
        "# Preprocess dataset\n",
        "directory = '/content/drive/MyDrive/Capstone/'\n",
        "train = pd.read_csv(directory + 'Data/train.csv')\n",
        "\n",
        "train['filepath'] = train['image'].apply(lambda x: f'{directory}Data/train_images/{x}').values\n",
        "\n",
        "# Create dictionary of items by label group\n",
        "label_dict = train.groupby('label_group')['posting_id'].unique().to_dict()\n",
        "\n",
        "# Create list of matching products\n",
        "train['matches'] = train['label_group'].map(label_dict)\n",
        "\n",
        "# Create dictionary of items by image_phash\n",
        "label_dict = train.groupby('image_phash')['posting_id'].unique().to_dict()\n",
        "\n",
        "# Create list of image duplicates by phash\n",
        "train['image_duplicates'] = train['image_phash'].map(label_dict)"
      ],
      "id": "-cVIWzzJFS3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zptm5sJ5RckK"
      },
      "source": [
        "Pre processing documents"
      ],
      "id": "zptm5sJ5RckK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bns5BbVwQQSL",
        "outputId": "c5f2eb70-776e-4396-ff88-7af5c4a4f244"
      },
      "source": [
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "id": "bns5BbVwQQSL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3KQnSDyRlhv"
      },
      "source": [
        "# Create function to clean text\n",
        "\n",
        "def tokenize_text(text, stop_words):\n",
        "\n",
        "    # Instantiate NLTK regextokenizer\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(pattern='\\w+')\n",
        "\n",
        "    # Create tokens\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    #clean_tokens = [w for w in tokens if w not in stop_words]\n",
        "    \n",
        "    return(' '.join(tokens))\n"
      ],
      "id": "D3KQnSDyRlhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ2xoxBYmt7h"
      },
      "source": [
        "# Create list of stop words\n",
        "stop_words = stopwords.words('indonesian') + stopwords.words('english')"
      ],
      "id": "tJ2xoxBYmt7h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azJh45daQQKG"
      },
      "source": [
        "# Clean text in new column\n",
        "\n",
        "token = train['title'].map(lambda x: tokenize_text(x, stop_words)).to_numpy()"
      ],
      "id": "azJh45daQQKG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33rUAbpfIAps"
      },
      "source": [
        "TFIDF Vectorizer Embeddings"
      ],
      "id": "33rUAbpfIAps"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqFeH945IDpO"
      },
      "source": [
        "tvec = TfidfVectorizer()"
      ],
      "id": "zqFeH945IDpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd7GIHsIIV-X"
      },
      "source": [
        "tfidf_embedding = tvec.fit_transform(token)\n"
      ],
      "id": "Qd7GIHsIIV-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOqgpNHyVVXN",
        "outputId": "735bc79e-d245-44f6-f88f-3040b5a71eaa"
      },
      "source": [
        "type(tfidf_embedding)"
      ],
      "id": "OOqgpNHyVVXN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztO2wVB0QEKm"
      },
      "source": [
        "sparse.save_npz(f'{directory}/Data/tfidf_embedding.npz', tfidf_embedding)"
      ],
      "id": "ztO2wVB0QEKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A19KPbXKPUs"
      },
      "source": [
        "LaBSE Embeddings"
      ],
      "id": "6A19KPbXKPUs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSpucd4KKyb"
      },
      "source": [
        "https://tfhub.dev/google/LaBSE/1"
      ],
      "id": "MLSpucd4KKyb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kltG0rocBEjX"
      },
      "source": [
        "def get_model(model_url, max_seq_length):\n",
        "  labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "\n",
        "  # Define input.\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                         name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                     name=\"input_mask\")\n",
        "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                      name=\"segment_ids\")\n",
        "\n",
        "  # LaBSE layer.\n",
        "  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "  # The embedding is l2 normalized.\n",
        "  pooled_output = tf.keras.layers.Lambda(\n",
        "      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
        "\n",
        "  # Define model.\n",
        "  return tf.keras.Model(\n",
        "        inputs=[input_word_ids, input_mask, segment_ids],\n",
        "        outputs=pooled_output), labse_layer\n"
      ],
      "id": "kltG0rocBEjX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kckX_5lFKqaC"
      },
      "source": [
        "max_seq_length = 64\n",
        "labse_model, labse_layer = get_model(\n",
        "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)"
      ],
      "id": "kckX_5lFKqaC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av0MuzdZKBqz"
      },
      "source": [
        "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "id": "av0MuzdZKBqz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3D18qb9K9QI"
      },
      "source": [
        "def create_input(input_strings, tokenizer, max_seq_length):\n",
        "\n",
        "  input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
        "  for input_string in input_strings:\n",
        "    # Tokenize input.\n",
        "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "    sequence_length = min(len(input_ids), max_seq_length)\n",
        "\n",
        "    # Padding or truncation.\n",
        "    if len(input_ids) >= max_seq_length:\n",
        "      input_ids = input_ids[:max_seq_length]\n",
        "    else:\n",
        "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "    input_ids_all.append(input_ids)\n",
        "    input_mask_all.append(input_mask)\n",
        "    segment_ids_all.append([0] * max_seq_length)\n",
        "\n",
        "  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\n"
      ],
      "id": "B3D18qb9K9QI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USkeMOtOK9y2"
      },
      "source": [
        "def encode(input_text):\n",
        "  input_ids, input_mask, segment_ids = create_input(\n",
        "    input_text, tokenizer, max_seq_length)\n",
        "  return labse_model([input_ids, input_mask, segment_ids])"
      ],
      "id": "USkeMOtOK9y2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk_YTfwURaH1"
      },
      "source": [
        "### Generate Word Embedding"
      ],
      "id": "Mk_YTfwURaH1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9pd-Jb-ROh5",
        "outputId": "3b55f767-7f56-479d-9f1d-f6011d3d7aaa"
      },
      "source": [
        "# To deal with the dataset size, we run the model in groups\n",
        "group_size = 1000\n",
        "groups = np.arange(np.ceil(len(train) / group_size))\n",
        "\n",
        "# Create empty list for embeddings\n",
        "embeddings = []\n",
        "\n",
        "for i in groups:\n",
        "  # Start and end index\n",
        "  start = int(i * group_size)\n",
        "  end = int((i + 1) * group_size)\n",
        "\n",
        "  # Get tokens\n",
        "  set_of_tokens = token[start:end]\n",
        "\n",
        "  # Generate embeddings\n",
        "  word_embeddings = encode(set_of_tokens)\n",
        "\n",
        "  # Append to embeddings list\n",
        "  embeddings.append(word_embeddings)\n",
        "\n",
        "  # Print status\n",
        "  print(f'Group {i} completed')\n",
        "\n",
        "train_word_embeddings = np.concatenate(embeddings)\n",
        "\n",
        "# Delete temporary variables to free memory\n",
        "del embeddings\n",
        "del set_of_tokens\n",
        "del word_embeddings"
      ],
      "id": "F9pd-Jb-ROh5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Group 0.0 completed\n",
            "Group 1.0 completed\n",
            "Group 2.0 completed\n",
            "Group 3.0 completed\n",
            "Group 4.0 completed\n",
            "Group 5.0 completed\n",
            "Group 6.0 completed\n",
            "Group 7.0 completed\n",
            "Group 8.0 completed\n",
            "Group 9.0 completed\n",
            "Group 10.0 completed\n",
            "Group 11.0 completed\n",
            "Group 12.0 completed\n",
            "Group 13.0 completed\n",
            "Group 14.0 completed\n",
            "Group 15.0 completed\n",
            "Group 16.0 completed\n",
            "Group 17.0 completed\n",
            "Group 18.0 completed\n",
            "Group 19.0 completed\n",
            "Group 20.0 completed\n",
            "Group 21.0 completed\n",
            "Group 22.0 completed\n",
            "Group 23.0 completed\n",
            "Group 24.0 completed\n",
            "Group 25.0 completed\n",
            "Group 26.0 completed\n",
            "Group 27.0 completed\n",
            "Group 28.0 completed\n",
            "Group 29.0 completed\n",
            "Group 30.0 completed\n",
            "Group 31.0 completed\n",
            "Group 32.0 completed\n",
            "Group 33.0 completed\n",
            "Group 34.0 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjsZHQPnROf0"
      },
      "source": [
        "# Save embeddings as npy file\n",
        "np.save(f'{directory}Data/labse_embeddings.npy', train_word_embeddings)"
      ],
      "id": "NjsZHQPnROf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdlGXO5udoIP"
      },
      "source": [
        ""
      ],
      "id": "CdlGXO5udoIP",
      "execution_count": null,
      "outputs": []
    }
  ]
}